<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hujunhan.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="1 IntroductionSpinning Up brought by OpenAI is a good resource for learning reinforcement learning. The goal of Spinning up is to ensure AI is safe developed and help people to learn Deep RL which has">
<meta property="og:type" content="article">
<meta property="og:title" content="ML:Reinforcement Learning">
<meta property="og:url" content="https://hujunhan.github.io/MarkdownNotes/CS/Machine%20Learning/ML-Reinforcement/index.html">
<meta property="og:site_name" content="Only Truth Matters">
<meta property="og:description" content="1 IntroductionSpinning Up brought by OpenAI is a good resource for learning reinforcement learning. The goal of Spinning up is to ensure AI is safe developed and help people to learn Deep RL which has">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/hujunhan/cloudimage/master/img/reinforcement-learning.png">
<meta property="og:image" content="https://hujunhan.github.io/rl_algorithms_9_15.svg">
<meta property="article:published_time" content="2019-04-15T22:54:00.000Z">
<meta property="article:modified_time" content="2021-10-18T03:53:51.000Z">
<meta property="article:author" content="Junhan Hu">
<meta property="article:tag" content="ml">
<meta property="article:tag" content="rl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/hujunhan/cloudimage/master/img/reinforcement-learning.png">

<link rel="canonical" href="https://hujunhan.github.io/MarkdownNotes/CS/Machine%20Learning/ML-Reinforcement/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ML:Reinforcement Learning | Only Truth Matters</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Only Truth Matters</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hujunhan.github.io/MarkdownNotes/CS/Machine%20Learning/ML-Reinforcement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Junhan Hu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Only Truth Matters">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML:Reinforcement Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-15 15:54:00" itemprop="dateCreated datePublished" datetime="2019-04-15T15:54:00-07:00">2019-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-17 20:53:51" itemprop="dateModified" datetime="2021-10-17T20:53:51-07:00">2021-10-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MarkdownNotes/" itemprop="url" rel="index"><span itemprop="name">MarkdownNotes</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MarkdownNotes/CS/" itemprop="url" rel="index"><span itemprop="name">CS</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MarkdownNotes/CS/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p><em><a target="_blank" rel="noopener" href="http://spinningup.openai.com/">Spinning Up</a></em> brought by OpenAI is a good resource for learning reinforcement learning.</p>
<p>The goal of Spinning up is to ensure AI is <strong>safe</strong> developed and help people to learn Deep RL which has a pretty high barrier to entry</p>
<blockquote>
<p>In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future.</p>
</blockquote>
<span id="more"></span>

<p>Learning Types：</p>
<ul>
<li>supervised：given x and y, find $f()$</li>
<li>unsupervised：given x, find $cluster$</li>
<li>reinforcement：given x and z, find decision $f()$ to generate y<ul>
<li>difference from supervised learning: <strong>delayed</strong> reward that is your movement will affect later world. about <strong>time</strong> and <strong>sequence</strong></li>
</ul>
</li>
</ul>
<p>Markov decision process (MDP) :</p>
<ul>
<li>states: things described the world</li>
<li>actions: things you can do</li>
<li>model: rules, physics of the world</li>
<li>reward: a scaler value to value the <em>state</em></li>
<li><strong>policy</strong>: what we want to learn, $\pi^{\star}(state)\to action$ ,the action can maximize the cumulative reward.<ul>
<li>when moving in the grid map, agent should avoid $-negative$ reward and get $+positive$ reward</li>
<li>You need to set the reward carefully  </li>
<li>Even if you are in the same state, your action will be different for different <strong>step(time)</strong> you can take later. E.g. if you have only 3 steps to go, you may take action <strong>risky</strong>, $\pi^{\star}(state,time)\to action$</li>
</ul>
</li>
</ul>
<h2 id="2-Key-Concepts"><a href="#2-Key-Concepts" class="headerlink" title="2 Key Concepts"></a>2 Key Concepts</h2><h3 id="2-1-What-can-RL-do"><a href="#2-1-What-can-RL-do" class="headerlink" title="2.1 What can RL do"></a>2.1 What can RL do</h3><ul>
<li>Teach computers to control robots in simulation and real world</li>
<li>Create breakthrough AI for sophisticated strategy games (Go&#x2F;Dota)</li>
<li>…</li>
</ul>
<h3 id="2-2-Terminology"><a href="#2-2-Terminology" class="headerlink" title="2.2 Terminology"></a>2.2 Terminology</h3><p>Main loop of RL:</p>
<p><img src="https://raw.githubusercontent.com/hujunhan/cloudimage/master/img/reinforcement-learning.png" alt="reinforcement-learning"></p>
<ul>
<li><p>state $s$ : <strong>complete</strong> description of the state of the world</p>
<ul>
<li>represented by a vector, matrix or tensor</li>
<li>Grid map e.g. $4\times3$</li>
</ul>
</li>
<li><p>observation $o$ : <strong>partial</strong> description of a state</p>
<ul>
<li>represented by a vector, matrix or tensor</li>
</ul>
</li>
<li><p>Action space: The set of all valid actions</p>
<ul>
<li>Discrete action space: Go and Atari</li>
<li>Continuous action space: Control a robot</li>
</ul>
</li>
<li><p>Policies: rule used by an agent to decide what actions to <strong>take</strong></p>
<ul>
<li>deterministic $a_{t}&#x3D;\mu\left(s_{t}\right)$</li>
<li>stochastic $a_{t} \sim \pi\left(\cdot | s_{t}\right)$ : sampling action $+$ computing $\log \pi_{\theta}(a | s)$ , <em>which means your action may not be execute $100%$</em> that is <strong>uncertainty</strong><ul>
<li>Catergorical policies: <strong>discrete</strong>, jsut like a <strong>classifier</strong>, output are probabilities vector</li>
<li>diagonal Gaussian policies: continuous</li>
</ul>
</li>
<li>The policy is trying to maximize reward</li>
</ul>
</li>
<li><p>Trajectories: $\ $ $\tau$ is a <strong>sequence of states and actions</strong> in the world</p>
<ul>
<li>$\tau&#x3D;\left(s_{0}, a_{0}, s_{1}, a_{1}, \dots\right)$</li>
</ul>
</li>
<li><p>Reward and Return: depends on the current state, the action just taken, and the next state of the world</p>
<ul>
<li>$r_{t}&#x3D;R\left(s_{t}, a_{t}, s_{t+1}\right)$  but ususally simplified to $r_{t}&#x3D;R\left(s_{t}\right)$</li>
<li>goal of the agent is to maximize cumulative reward over trajectory $R(\tau)&#x3D;\sum_{t&#x3D;0}^{T} r_{t}$</li>
<li>if the time is infinite, we should add a <strong>factor</strong> to the cumulative reward or it will go $\infty$ , so we should $R(\tau)&#x3D;\sum_{t&#x3D;0}^{\infty} \gamma^{t} r_{t}$</li>
</ul>
</li>
<li><p>The RL Problem: select a policy which maximized <strong>expected return</strong></p>
</li>
<li><p>Value Funciton: know the value of a state</p>
<ol>
<li><p><strong>On-Policy Value Function</strong> $V^{\pi}(s)$</p>
</li>
<li><p><strong>On-Policy Action-Value Function</strong> $Q^{\pi}(s, a)$</p>
</li>
</ol>
<p> start with random action</p>
<ol start="3">
<li><p><strong>Optimal Value Function</strong> $V^{*}(s) $</p>
</li>
<li><p><strong>Optimal Action-Value Function</strong> $Q^{*}(s, a)$</p>
</li>
</ol>
</li>
<li><p>Bellman Equations</p>
<ul>
<li><blockquote>
<p>basic idea: The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.</p>
</blockquote>
</li>
<li><p>$V^{*}(s)&#x3D;\max <em>{\pi} \underset{\tau \sim \pi}{\mathrm{E}}\left[R(\tau) | s</em>{0}&#x3D;s\right]$</p>
<p>$n$ equations and $n$ unknown</p>
</li>
</ul>
</li>
</ul>
<h2 id="3-Kinds-of-RL-Algorithms"><a href="#3-Kinds-of-RL-Algorithms" class="headerlink" title="3 Kinds of RL Algorithms"></a>3 Kinds of RL Algorithms</h2><h3 id="3-1-Taxonomy"><a href="#3-1-Taxonomy" class="headerlink" title="3.1 Taxonomy"></a>3.1 Taxonomy</h3><p><img src="/rl_algorithms_9_15.svg"></p>
<p>Model-Based RL:</p>
<ul>
<li>Upside: allows the agent to plan by thinking ahead. Example: AlphaZero</li>
<li>Downside: the given model is different from the real enviroment the agent works.</li>
</ul>
<p>Model-Free RL:</p>
<ul>
<li>Upside: easier to implement and tune, more popular</li>
</ul>
<h3 id="3-2-What-to-Learn"><a href="#3-2-What-to-Learn" class="headerlink" title="3.2 What to Learn"></a>3.2 What to Learn</h3><h4 id="3-2-1-Model-Free-RL"><a href="#3-2-1-Model-Free-RL" class="headerlink" title="3.2.1 Model-Free RL"></a>3.2.1 Model-Free RL</h4><ul>
<li><p>Policy Optimization: On policy</p>
<ul>
<li>A2C &#x2F; A3C, gradient ascent</li>
<li>PPO,  updates indirectly</li>
</ul>
</li>
<li><p>Q-Learning: Off policy</p>
<p>learn an approximator $Q_{\theta}(s, a)$ for the optiaml action-value function.</p>
<ul>
<li>DQN</li>
<li>C51</li>
</ul>
</li>
<li><p>Trade-offs:</p>
<ul>
<li>Policy: these kind of algorithm optimize for the thing we want directly.</li>
<li>Q-Learning’s result is unstable, which means it may be better sometimes but maybe worse other times.</li>
</ul>
</li>
</ul>
<h4 id="3-2-2-Model-Based-RL"><a href="#3-2-2-Model-Based-RL" class="headerlink" title="3.2.2 Model-Based RL"></a>3.2.2 Model-Based RL</h4><ul>
<li>Pure Planning: compute a new plan each time, execute the first action and discards the rest of it.</li>
<li>Expert Iteration: updated version of Pure-Planning</li>
<li>Data Augmentation for Model-Free Methods</li>
<li>…</li>
</ul>
<h2 id="4-Policy-Optimization"><a href="#4-Policy-Optimization" class="headerlink" title="4 Policy Optimization"></a>4 Policy Optimization</h2><h3 id="4-1-Deriving-some-Math"><a href="#4-1-Deriving-some-Math" class="headerlink" title="4.1 Deriving some Math"></a>4.1 Deriving some Math</h3><p>Here we consider the case of stochastic, parameterized policy.</p>
<p><strong>Aim</strong>: maximize the expected return $J\left(\pi_{\theta}\right)&#x3D;\underset{\tau \sim \pi_{\theta}}{E}[R(\tau)]$</p>
<p><strong>How</strong>: optimize by gradient ascent: $\theta_{k+1}&#x3D;\theta_{k}+\alpha \nabla_{\theta} J\left.\left(\pi_{\theta}\right)\right|<em>{\theta</em>{k}}$  ,which means we need an expression for the policy.</p>
<p><strong>Program</strong>:</p>
<ol>
<li>Making the policy network<ol>
<li>make core policy network</li>
<li>make action selection</li>
</ol>
</li>
<li>Making the Loss Function</li>
<li>Running one Epoch of Training</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ml/" rel="tag"># ml</a>
              <a href="/tags/rl/" rel="tag"># rl</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/MarkdownNotes/Robotics/Project/flight-controller/" rel="prev" title="Flight Controller">
      <i class="fa fa-chevron-left"></i> Flight Controller
    </a></div>
      <div class="post-nav-item">
    <a href="/MarkdownNotes/CS/Machine%20Learning/ML-Transfer/" rel="next" title="ML:Transfer Learning">
      ML:Transfer Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Key-Concepts"><span class="nav-text">2 Key Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-What-can-RL-do"><span class="nav-text">2.1 What can RL do</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Terminology"><span class="nav-text">2.2 Terminology</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Kinds-of-RL-Algorithms"><span class="nav-text">3 Kinds of RL Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Taxonomy"><span class="nav-text">3.1 Taxonomy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-What-to-Learn"><span class="nav-text">3.2 What to Learn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-Model-Free-RL"><span class="nav-text">3.2.1 Model-Free RL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-Model-Based-RL"><span class="nav-text">3.2.2 Model-Based RL</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Policy-Optimization"><span class="nav-text">4 Policy Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Deriving-some-Math"><span class="nav-text">4.1 Deriving some Math</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Junhan Hu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">260</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">98</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hujunhan" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hujunhan" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hujunhan98@outlook.com" title="E-Mail → mailto:hujunhan98@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Junhan Hu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
